{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Gemma 3 1B untuk Chatbot PMB\n",
    "## Tugas Akhir: Pengembangan Chatbot Penerimaan Mahasiswa Baru\n",
    "\n",
    "**Metode:** QLoRA (Quantized Low-Rank Adaptation)  \n",
    "**Base Model:** Google Gemma 3 1B Instruct  \n",
    "**Dataset:** PMB Universitas Sains Al-Qur'an  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bert-score nltk rouge-score pandas torch matplotlib seaborn pyyaml numpy scikit-learn transformers datasets trl peft bitsandbytes accelerate sentencepiece protobuf torch torchvision scikit-learn wordcloud\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style untuk grafik\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Ukuran default untuk gambar\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üêç Python: {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "print(f\"üíª CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 1. Analisis Dataset\n",
    "\n",
    "### 1.1 Load Dataset\n",
    "\n",
    "**Pilih salah satu metode:**\n",
    "- **Method A (cell berikutnya):** Auto-split dari file augmented (Recommended)\n",
    "- **Method B (cell setelahnya):** Load dari file yang sudah di-split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1b Load dari File Terpisah (Alternative Method)\n",
    "\n",
    "**Gunakan metode ini jika Anda sudah punya file train/val yang terpisah.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1a Auto-Split Dataset (Recommended)\n",
    "\n",
    "**Metode ini akan load dataset utama dan auto-split menjadi train/eval/test dengan ratio yang ditentukan.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ============================================================================\n",
    "# KONFIGURASI\n",
    "# ============================================================================\n",
    "\n",
    "HF_TOKEN = \"your_code\"\n",
    "\n",
    "# File paths\n",
    "INPUT_FILE = \"dataset_v4.txt\"\n",
    "JSON_FILE = \"dataset_v2.json\"\n",
    "FORMATTED_FILE = \"dataset_formatted.json\"\n",
    "\n",
    "# Output directories\n",
    "DATA_DIR = \"data\"\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"train_pmb.json\")\n",
    "EVAL_FILE = os.path.join(DATA_DIR, \"eval_pmb.json\")\n",
    "TEST_FILE = os.path.join(DATA_DIR, \"test_pmb.json\")\n",
    "\n",
    "# Split ratios\n",
    "TRAIN_RATIO = 0.8   # 70%\n",
    "EVAL_RATIO = 0.1   # 15%\n",
    "TEST_RATIO = 0.1   # 15%\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = (\n",
    "    \"Anda adalah asisten virtual untuk Penerimaan Mahasiswa Baru (PMB) di \"\n",
    "    \"Universitas Sains Al-Qur'an (UNSIQ) Wonosobo.\\n\"\n",
    "   \n",
    ")\n",
    "\n",
    "METADATA = {\n",
    "    \"topic\": \"PMB UNSIQ\",\n",
    "    \"subtopic\": \"Informasi Umum\"\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: CONVERT TXT TO JSON\n",
    "# ============================================================================\n",
    "\n",
    "def convert_txt_to_json():\n",
    "    \"\"\"Konversi dataset_v2.txt ke JSON\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 1: Converting TXT to JSON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    data = []\n",
    "    entry = {}\n",
    "    \n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            if line.lower().startswith(\"q:\"):\n",
    "                entry = {\"question\": line[2:].strip()}\n",
    "            elif line.lower().startswith(\"a:\"):\n",
    "                entry[\"answer\"] = line[2:].strip()\n",
    "                data.append(entry)\n",
    "                entry = {}\n",
    "    \n",
    "    # Simpan ke JSON\n",
    "    with open(JSON_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Konversi selesai!\")\n",
    "    print(f\"   Total Q&A pairs: {len(data)}\")\n",
    "    print(f\"   File tersimpan: {JSON_FILE}\\n\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: FORMAT DATASET\n",
    "# ============================================================================\n",
    "\n",
    "def format_dataset(data):\n",
    "    \"\"\"Format dataset dengan Gemma chat template\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 2: Formatting Dataset\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    formatted_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        question = item.get(\"question\", \"\")\n",
    "        answer = item.get(\"answer\", \"\")\n",
    "        \n",
    "        formatted_item = {\n",
    "            \"text\": (\n",
    "                f\"<start_of_turn>system\\n{SYSTEM_PROMPT}<end_of_turn>\\n\"\n",
    "                f\"<start_of_turn>user\\n{question}<end_of_turn>\\n\"\n",
    "                f\"<start_of_turn>model\\n{answer}<end_of_turn>\"\n",
    "            ),\n",
    "       \n",
    "        }\n",
    "        formatted_data.append(formatted_item)\n",
    "    \n",
    "    # Simpan formatted dataset\n",
    "    with open(FORMATTED_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(formatted_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Formatting selesai!\")\n",
    "    print(f\"   Total formatted data: {len(formatted_data)}\")\n",
    "    print(f\"   File tersimpan: {FORMATTED_FILE}\\n\")\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: SPLIT DATASET\n",
    "# ============================================================================\n",
    "\n",
    "def split_dataset(data):\n",
    "    \"\"\"Split dataset menjadi train, eval, dan test\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 3: Splitting Dataset\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total = len(data)\n",
    "    print(f\"Total data: {total} samples\")\n",
    "    print(f\"Split ratio: Train={TRAIN_RATIO*100:.0f}%, Eval={EVAL_RATIO*100:.0f}%, Test={TEST_RATIO*100:.0f}%\\n\")\n",
    "    \n",
    "    # Split train dan temp (eval + test)\n",
    "    train_data, temp_data = train_test_split(\n",
    "        data,\n",
    "        test_size=(1 - TRAIN_RATIO),\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Split eval dan test\n",
    "    eval_size = EVAL_RATIO / (EVAL_RATIO + TEST_RATIO)\n",
    "    eval_data, test_data = train_test_split(\n",
    "        temp_data,\n",
    "        test_size=(1 - eval_size),\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Buat direktori jika belum ada\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    # Simpan masing-masing split\n",
    "    with open(TRAIN_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    with open(EVAL_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(eval_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    with open(TEST_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Split selesai!\")\n",
    "    print(f\"   ‚Ä¢ Training:   {len(train_data):4d} samples ({len(train_data)/total*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Evaluation: {len(eval_data):4d} samples ({len(eval_data)/total*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Test:       {len(test_data):4d} samples ({len(test_data)/total*100:.1f}%)\")\n",
    "    print(f\"\\nüíæ Files tersimpan:\")\n",
    "    print(f\"   ‚Ä¢ {TRAIN_FILE}\")\n",
    "    print(f\"   ‚Ä¢ {EVAL_FILE}\")\n",
    "    print(f\"   ‚Ä¢ {TEST_FILE}\\n\")\n",
    "    \n",
    "    return train_data, eval_data, test_data\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET PREPARATION PIPELINE\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Login ke Hugging Face\n",
    "print(\"üîê Logging in to Hugging Face...\")\n",
    "login(HF_TOKEN)\n",
    "print(\"‚úÖ Login berhasil!\\n\")\n",
    "\n",
    "# Step 1: Convert TXT to JSON\n",
    "json_data = convert_txt_to_json()\n",
    "\n",
    "# Step 2: Format dataset\n",
    "formatted_data = format_dataset(json_data)\n",
    "\n",
    "# Step 3: Split dataset\n",
    "train_data, eval_data, test_data = split_dataset(formatted_data)\n",
    "\n",
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"üéâ SEMUA PROSES SELESAI!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä Total data: {len(formatted_data)} samples\")\n",
    "print(f\"‚úÖ Training set:   {len(train_data)} samples\")\n",
    "print(f\"‚úÖ Evaluation set: {len(eval_data)} samples\")\n",
    "print(f\"‚úÖ Test set:       {len(test_data)} samples\")\n",
    "print(\"\\nDataset siap untuk training! üöÄ\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "with open('data/train_pmb.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load evaluation data\n",
    "with open('data/eval_pmb.json', 'r', encoding='utf-8') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Load test data\n",
    "with open('data/test_pmb.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"üìä Dataset Statistics:\")\n",
    "print(f\"  Training samples:   {len(train_data)}\")\n",
    "print(f\"  Evaluation samples: {len(eval_data)}\")\n",
    "print(f\"  Test samples:       {len(test_data)}\")\n",
    "print(f\"  Total samples:      {len(train_data) + len(eval_data) + len(test_data)}\")\n",
    "\n",
    "total = len(train_data) + len(eval_data) + len(test_data)\n",
    "print(f\"\\nüìà Split Ratio:\")\n",
    "print(f\"  Train: {len(train_data)/total*100:.1f}%\")\n",
    "print(f\"  Eval:  {len(eval_data)/total*100:.1f}%\")\n",
    "print(f\"  Test:  {len(test_data)/total*100:.1f}%\")\n",
    "\n",
    "# Untuk compatibility dengan kode lain yang menggunakan val_data\n",
    "val_data = eval_data\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Distribusi Panjang Teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Analisis panjang teks\n",
    "train_lengths = [len(item['text'].split()) for item in train_data]\n",
    "val_lengths = [len(item['text'].split()) for item in val_data]\n",
    "\n",
    "# Statistik\n",
    "print(\"üìè Text Length Statistics (in words):\")\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  Mean:   {np.mean(train_lengths):.2f}\")\n",
    "print(f\"  Median: {np.median(train_lengths):.2f}\")\n",
    "print(f\"  Min:    {np.min(train_lengths)}\")\n",
    "print(f\"  Max:    {np.max(train_lengths)}\")\n",
    "print(f\"  Std:    {np.std(train_lengths):.2f}\")\n",
    "\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  Mean:   {np.mean(val_lengths):.2f}\")\n",
    "print(f\"  Median: {np.median(val_lengths):.2f}\")\n",
    "print(f\"  Min:    {np.min(val_lengths)}\")\n",
    "print(f\"  Max:    {np.max(val_lengths)}\")\n",
    "print(f\"  Std:    {np.std(val_lengths):.2f}\")\n",
    "\n",
    "# Buat direktori output jika belum ada\n",
    "output_dir = 'outputs/figures'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Visualisasi\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(train_lengths, bins=30, alpha=0.7, label='Training', color='#2ecc71', edgecolor='black')\n",
    "ax1.hist(val_lengths, bins=30, alpha=0.7, label='Validation', color='#3498db', edgecolor='black')\n",
    "ax1.set_xlabel('Jumlah Kata', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Frekuensi', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribusi Panjang Teks Dataset PMB', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot (perbaikan parameter labels -> tick_labels)\n",
    "bp = ax2.boxplot([train_lengths, val_lengths], \n",
    "                  tick_labels=['Training', 'Validation'],  # Perbaikan di sini\n",
    "                  patch_artist=True,\n",
    "                  boxprops=dict(facecolor='#3498db', alpha=0.7),\n",
    "                  medianprops=dict(color='red', linewidth=2))\n",
    "ax2.set_ylabel('Jumlah Kata', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Box Plot Panjang Teks', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Simpan gambar (path diperbaiki)\n",
    "output_path = os.path.join(output_dir, 'dataset_distribution.png')\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan 3 contoh data\n",
    "print(\"üìù Contoh Data Training:\")\n",
    "print(\"=\"*80)\n",
    "for i, sample in enumerate(train_data[:3], 1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"{sample['text'][:200]}...\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open('../configs/qlora_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Display config\n",
    "print(\"‚öôÔ∏è  Training Configuration:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüì¶ Model:\")\n",
    "print(f\"  Base Model: {config['model_config']['model_name']}\")\n",
    "print(f\"\\nüîß LoRA Config:\")\n",
    "print(f\"  Rank (r): {config['qlora_config']['r']}\")\n",
    "print(f\"  Alpha: {config['qlora_config']['lora_alpha']}\")\n",
    "print(f\"  Dropout: {config['qlora_config']['lora_dropout']}\")\n",
    "print(f\"\\nüéì Training Args:\")\n",
    "print(f\"  Epochs: {config['training_args']['num_train_epochs']}\")\n",
    "print(f\"  Batch Size: {config['training_args']['per_device_train_batch_size']}\")\n",
    "print(f\"  Learning Rate: {config['training_args']['learning_rate']}\")\n",
    "print(f\"  Gradient Accumulation: {config['training_args']['gradient_accumulation_steps']}\")\n",
    "print(f\"  Effective Batch Size: {config['training_args']['per_device_train_batch_size'] * config['training_args']['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 2. Training Model\n",
    "\n",
    "### 2.1 Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ‚öôÔ∏è SETUP KONFIGURASI QLORA (TANPA TRAINING)\n",
    "# ============================================================\n",
    "\n",
    "import os, json\n",
    "\n",
    "config = {\n",
    "    # ========== MODEL CONFIGURATION ==========\n",
    "    'model_config': {\n",
    "        'model_name': 'google/gemma-3-1b-it',  # ‚Üê GANTI di sini untuk model lain\n",
    "        'use_cache': False,\n",
    "        'trust_remote_code': True,\n",
    "        'torch_dtype': 'bfloat16',  # bfloat16 optimal untuk GPU A100\n",
    "    },\n",
    "    \n",
    "    # ========== QUANTIZATION CONFIG (QLoRA) ==========\n",
    "    'quantization_config': {\n",
    "        'load_in_4bit': True,\n",
    "        'bnb_4bit_compute_dtype': 'bfloat16',\n",
    "        'bnb_4bit_quant_type': 'nf4',\n",
    "        'bnb_4bit_use_double_quant': True,\n",
    "    },\n",
    "    \n",
    "    # ========== LORA CONFIGURATION ==========\n",
    "    'qlora_config': {\n",
    "        'r': 16,                    # LoRA rank (8‚Äì64)\n",
    "        'lora_alpha': 32,           # scaling factor (biasanya 2x r)\n",
    "        'lora_dropout': 0.05,\n",
    "        'bias': 'none',\n",
    "        'task_type': 'CAUSAL_LM',\n",
    "        'target_modules': [\n",
    "            'q_proj', 'k_proj', 'v_proj',\n",
    "            'o_proj', 'gate_proj', 'up_proj', 'down_proj'\n",
    "        ],\n",
    "    },\n",
    "    \n",
    "    # ========== DATASET CONFIG ==========\n",
    "    'dataset_config': {\n",
    "        'train_file': 'data/train_pmb.json',   # ‚Üê path dataset train\n",
    "        'eval_file': 'data/eval_pmb.json',      # ‚Üê path dataset eval\n",
    "        'max_length': 512,\n",
    "        'text_field': 'text',\n",
    "    },\n",
    "    \n",
    "    # ========== TRAINING ARGUMENTS (A100 80GB) ==========\n",
    "    'training_args': {\n",
    "        'output_dir': '../outputs/gemma-pmb',\n",
    "        'overwrite_output_dir': True,\n",
    "        'num_train_epochs': 3,\n",
    "        'per_device_train_batch_size': 8,\n",
    "        'per_device_eval_batch_size': 8,\n",
    "        'gradient_accumulation_steps': 4,\n",
    "        'gradient_checkpointing': True,\n",
    "        'optim': 'paged_adamw_8bit',\n",
    "        'learning_rate': 2e-4,\n",
    "        'weight_decay': 0.01,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'lr_scheduler_type': 'cosine',\n",
    "        'warmup_ratio': 0.03,\n",
    "        'eval_strategy': 'epoch',\n",
    "        'eval_steps': 100,\n",
    "        'save_strategy': 'epoch',\n",
    "        'save_total_limit': 2,\n",
    "        'load_best_model_at_end': True,\n",
    "        'metric_for_best_model': 'eval_loss',\n",
    "        'logging_strategy': 'steps',\n",
    "        'logging_steps': 10,\n",
    "        'report_to': 'none',\n",
    "        'bf16': True,\n",
    "        'bf16_full_eval': True,\n",
    "        'dataloader_num_workers': 4,\n",
    "        'group_by_length': True,\n",
    "        'ddp_find_unused_parameters': False,\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# üíæ SIMPAN CONFIG KE FILE\n",
    "# ============================================================\n",
    "\n",
    "os.makedirs('../configs', exist_ok=True)\n",
    "config_file = '../configs/qlora_config.json'\n",
    "\n",
    "with open(config_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ============================================================\n",
    "# üßæ CETAK RINGKASAN\n",
    "# ============================================================\n",
    "\n",
    "print(\"‚öôÔ∏è  KONFIGURASI QLORA - NVIDIA A100 80GB\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüì¶ MODEL:\\n  Model Name      : {config['model_config']['model_name']}\")\n",
    "print(f\"  Precision       : {config['model_config']['torch_dtype']}\")\n",
    "print(f\"  Quantization    : 4-bit NF4 (QLoRA)\")\n",
    "\n",
    "print(f\"\\nüîß LORA CONFIG:\")\n",
    "print(f\"  Rank (r)        : {config['qlora_config']['r']}\")\n",
    "print(f\"  Alpha           : {config['qlora_config']['lora_alpha']}\")\n",
    "print(f\"  Dropout         : {config['qlora_config']['lora_dropout']}\")\n",
    "print(f\"  Target Modules  : {len(config['qlora_config']['target_modules'])} modules\")\n",
    "\n",
    "print(f\"\\nüìä DATASET:\")\n",
    "print(f\"  Train File      : {config['dataset_config']['train_file']}\")\n",
    "print(f\"  Eval File       : {config['dataset_config']['eval_file']}\")\n",
    "print(f\"  Max Length      : {config['dataset_config']['max_length']} tokens\")\n",
    "\n",
    "print(f\"\\nüéì TRAINING PARAMETERS:\")\n",
    "print(f\"  Epochs          : {config['training_args']['num_train_epochs']}\")\n",
    "print(f\"  Batch Size      : {config['training_args']['per_device_train_batch_size']}\")\n",
    "print(f\"  Gradient Accum  : {config['training_args']['gradient_accumulation_steps']}\")\n",
    "print(f\"  Effective Batch : {config['training_args']['per_device_train_batch_size'] * config['training_args']['gradient_accumulation_steps']}\")\n",
    "print(f\"  Learning Rate   : {config['training_args']['learning_rate']}\")\n",
    "print(f\"  Optimizer       : {config['training_args']['optim']}\")\n",
    "\n",
    "print(f\"\\nüíæ MEMORY OPTIMIZATION:\")\n",
    "print(\"  4-bit Quantization      : ‚úÖ\")\n",
    "print(\"  Double Quantization     : ‚úÖ\")\n",
    "print(\"  Gradient Checkpointing  : ‚úÖ\")\n",
    "print(\"  Paged AdamW 8-bit       : ‚úÖ\")\n",
    "\n",
    "print(f\"\\nüíæ Config saved to: {config_file}\")\n",
    "print(\"\\nüí° TIPS:\")\n",
    "print(\"  ‚Ä¢ Ganti model: ubah 'model_name' lalu jalankan ulang cell ini\")\n",
    "print(\"  ‚Ä¢ Untuk gemma-2-4b-it: model_name = 'google/gemma-2-4b-it'\")\n",
    "print(\"  ‚Ä¢ Model besar? Kurangi batch_size atau tambah gradient_accumulation\")\n",
    "print(\"\\n‚úÖ Konfigurasi siap! Lanjut ke tahap berikutnya bila ingin training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 TRAINING PROSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING QLORA - FINAL VERSION\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "\n",
    "print(\"üöÄ STARTING QLORA TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ========== LOAD CONFIG ==========\n",
    "with open(\"../configs/qlora_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# ========== 1. LOAD TOKENIZER ==========\n",
    "print(\"\\nüì• Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config['model_config']['model_name'],\n",
    "    trust_remote_code=config['model_config']['trust_remote_code']\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded: {config['model_config']['model_name']}\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# ========== 2. LOAD DATASET ==========\n",
    "print(\"\\nüìä Loading dataset...\")\n",
    "with open(config['dataset_config']['train_file'], 'r', encoding='utf-8') as f:\n",
    "    train_data_raw = json.load(f)\n",
    "with open(config['dataset_config']['eval_file'], 'r', encoding='utf-8') as f:\n",
    "    eval_data_raw = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded: {len(train_data_raw)} train, {len(eval_data_raw)} eval\")\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data_raw)\n",
    "eval_dataset = Dataset.from_list(eval_data_raw)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[config['dataset_config']['text_field']],\n",
    "        truncation=True,\n",
    "        max_length=config['dataset_config']['max_length'],\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "print(\"\\nüîÑ Tokenizing...\")\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True, remove_columns=eval_dataset.column_names)\n",
    "print(\"‚úÖ Tokenization done\")\n",
    "\n",
    "# ========== 3. LOAD MODEL (4-BIT) ==========\n",
    "print(\"\\nüì¶ Loading model with 4-bit quantization...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config['model_config']['model_name'],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded, Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# ========== 4. ADD LORA ==========\n",
    "print(\"\\nüîß Adding LoRA adapters...\")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=config['qlora_config']['r'],\n",
    "    lora_alpha=config['qlora_config']['lora_alpha'],\n",
    "    lora_dropout=config['qlora_config']['lora_dropout'],\n",
    "    bias=config['qlora_config']['bias'],\n",
    "    task_type=config['qlora_config']['task_type'],\n",
    "    target_modules=config['qlora_config']['target_modules']\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ========== 5. TRAINER SETUP ==========\n",
    "print(\"\\n‚öôÔ∏è  Setting up trainer...\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"{config['training_args']['output_dir']}_{timestamp}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=config['training_args']['num_train_epochs'],\n",
    "    per_device_train_batch_size=config['training_args']['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=config['training_args']['per_device_eval_batch_size'],\n",
    "    gradient_accumulation_steps=config['training_args']['gradient_accumulation_steps'],\n",
    "    gradient_checkpointing=True,\n",
    "    optim=config['training_args']['optim'],\n",
    "    learning_rate=config['training_args']['learning_rate'],\n",
    "    weight_decay=config['training_args']['weight_decay'],\n",
    "    max_grad_norm=config['training_args']['max_grad_norm'],\n",
    "    lr_scheduler_type=config['training_args']['lr_scheduler_type'],\n",
    "    warmup_ratio=config['training_args']['warmup_ratio'],\n",
    "    eval_strategy=config['training_args']['eval_strategy'],   # ‚Üê diperbaiki\n",
    "    save_strategy=config['training_args']['save_strategy'],\n",
    "    save_total_limit=config['training_args']['save_total_limit'],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=config['training_args']['logging_steps'],\n",
    "    report_to='none',\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    dataloader_num_workers=config['training_args']['dataloader_num_workers'],\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Trainer ready, Output: {output_dir}\")\n",
    "\n",
    "# ========== 6. TRAIN ==========\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéì TRAINING START...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ TRAINING DONE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"‚è±Ô∏è  Time: {duration/60:.2f} min\")\n",
    "    print(f\"üìâ Final loss: {train_result.metrics.get('train_loss', 0):.4f}\")\n",
    "\n",
    "    # Save model and metrics\n",
    "    print(\"\\nüíæ Saving...\")\n",
    "    # Simpan state training\n",
    "    trainer.save_state()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    with open(f\"{output_dir}/training_metrics.json\", 'w') as f:\n",
    "        json.dump({\n",
    "            'train_loss': float(train_result.metrics.get('train_loss', 0)),\n",
    "            'duration_minutes': duration / 60,\n",
    "            'config': config\n",
    "        }, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Saved to: {output_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 MERGER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üß© MERGE QLORA ADAPTER DENGAN MODEL BASE (FINAL CODE)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- Path konfigurasi ---\n",
    "base_model_name = \"google/gemma-3-1b-it\"                     # model base\n",
    "adapter_path = \"../outputs/gemma-pmb_20251108_112804\"        # hasil training LoRA\n",
    "merged_model_path = \"../outputs/gemma-pmb_merged_final_v2\"      # output merge\n",
    "\n",
    "# --- 1Ô∏è‚É£ Load base model & tokenizer ---\n",
    "print(f\"üì¶ Loading base model: {base_model_name}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(f\"üì• Loading tokenizer from base model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# --- 2Ô∏è‚É£ Load adapter & merge ke model base ---\n",
    "print(\"\\nüîÑ Loading LoRA adapters and merging...\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "torch.cuda.empty_cache()\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# --- 3Ô∏è‚É£ Simpan model hasil merge ---\n",
    "print(\"\\nüíæ Saving merged model...\")\n",
    "model.save_pretrained(merged_model_path, safe_serialization=True)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Merge complete!\")\n",
    "print(f\"üíæ Final merged model saved to: {merged_model_path}\")\n",
    "print(f\"üìè Vocab size: {model.config.vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 INFERENCE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ü§ñ INFERENCE (Prompt Engineered) - GEMMA 3 1B QLORA (MERGED)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# --- Path model hasil merge ---\n",
    "model_dir = \"../outputs/gemma-pmb_merged_final_v2\"\n",
    "\n",
    "# --- 1Ô∏è‚É£ Load model & tokenizer ---\n",
    "print(f\"üì¶ Loading merged model from: {model_dir}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# --- 2Ô∏è‚É£ Template Prompt (mengikuti struktur dataset kamu) ---\n",
    "SYSTEM_PROMPT = (\n",
    "    \"<start_of_turn>system \"\n",
    "    \"Anda adalah asisten virtual untuk Penerimaan Mahasiswa Baru (PMB) di Universitas Sains Al-Qur'an (UNSIQ) Wonosobo. \"\n",
    "    \"<end_of_turn>\"\n",
    ")\n",
    "\n",
    "def build_prompt(user_question: str):\n",
    "    return (\n",
    "        f\"{SYSTEM_PROMPT}\\n\"\n",
    "        f\"<start_of_turn>user {user_question}<end_of_turn>\\n\"\n",
    "        f\"<start_of_turn>model \"\n",
    "    )\n",
    "\n",
    "# --- 3Ô∏è‚É£ Masukkan pertanyaan ---\n",
    "user_question = \"fasilitas apa saja yang ada di unsiq?\"\n",
    "prompt = build_prompt(user_question)\n",
    "\n",
    "# --- 4Ô∏è‚É£ Tokenisasi & generate ---\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Generating output...\")\n",
    "with torch.inference_mode():\n",
    "    output_tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# --- 5Ô∏è‚É£ Decode hasil ---\n",
    "result = tokenizer.decode(output_tokens[0], skip_special_tokens=False)\n",
    "\n",
    "# Ambil hanya bagian setelah \"<start_of_turn>model\"\n",
    "if \"<start_of_turn>model\" in result:\n",
    "    result = result.split(\"<start_of_turn>model\")[-1]\n",
    "if \"<end_of_turn>\" in result:\n",
    "    result = result.split(\"<end_of_turn>\")[0]\n",
    "\n",
    "print(\"\\nüß† Model Response:\")\n",
    "print(\"=\" * 80)\n",
    "print(result.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script untuk test model dengan pertanyaan dari test_pmb.json\n",
    "Format data: Gemma chat template dengan user dan model turns\n",
    "Evaluasi dengan BERT Score\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "MODEL_PATH = \"../outputs/gemma-pmb_merged_final\"\n",
    "TEST_DATA_PATH = \"data/test_pmb.json\"  # Path ke file test data\n",
    "OUTPUT_DIR = \"../outputs\"\n",
    "\n",
    "# System prompt untuk model\n",
    "SYSTEM_PROMPT = \"\"\"Anda adalah asisten virtual untuk Penerimaan Mahasiswa Baru (PMB) di Universitas Sains Al-Qur'an.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"  LOADING MODEL AND TEST DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìÇ Loading model dari: {MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"‚úÖ Model berhasil di-load!\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD TEST DATA\n",
    "# ============================================================================\n",
    "print(f\"\\nüìÇ Loading test data dari: {TEST_DATA_PATH}\")\n",
    "\n",
    "def extract_qa_from_text(text):\n",
    "    \"\"\"Extract question dan answer dari format Gemma chat template\"\"\"\n",
    "    try:\n",
    "        # Extract user question\n",
    "        if \"<start_of_turn>user\\n\" in text and \"<end_of_turn>\" in text:\n",
    "            user_start = text.find(\"<start_of_turn>user\\n\") + len(\"<start_of_turn>user\\n\")\n",
    "            user_end = text.find(\"<end_of_turn>\", user_start)\n",
    "            question = text[user_start:user_end].strip()\n",
    "        else:\n",
    "            question = \"\"\n",
    "        \n",
    "        # Extract model answer (reference)\n",
    "        if \"<start_of_turn>model\\n\" in text:\n",
    "            model_start = text.find(\"<start_of_turn>model\\n\") + len(\"<start_of_turn>model\\n\")\n",
    "            model_end = text.find(\"<end_of_turn>\", model_start)\n",
    "            if model_end == -1:\n",
    "                # Jika tidak ada closing tag, ambil sampai akhir\n",
    "                reference = text[model_start:].strip()\n",
    "            else:\n",
    "                reference = text[model_start:model_end].strip()\n",
    "        else:\n",
    "            reference = \"\"\n",
    "        \n",
    "        return question, reference\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error extracting Q&A: {e}\")\n",
    "        return \"\", \"\"\n",
    "\n",
    "try:\n",
    "    with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "        test_data_raw = json.load(f)\n",
    "    \n",
    "    # Convert format ke yang dibutuhkan\n",
    "    test_data = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for idx, item in enumerate(test_data_raw, 1):\n",
    "        text = item.get(\"text\", \"\")\n",
    "        \n",
    "        if not text:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Extract question dan reference dari text\n",
    "        question, reference = extract_qa_from_text(text)\n",
    "        \n",
    "        if not question or not reference:\n",
    "            print(f\"‚ö†Ô∏è  Sample #{idx} skipped - empty Q or A\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        test_data.append({\n",
    "            \"question\": question,\n",
    "            \"reference\": reference,\n",
    "            \"original_text\": text\n",
    "        })\n",
    "    \n",
    "    print(f\"‚úÖ Test data berhasil di-load!\")\n",
    "    print(f\"üìä Total pertanyaan: {len(test_data)}\")\n",
    "    print(f\"‚ö†Ô∏è  Skipped: {skipped}\")\n",
    "    \n",
    "    if len(test_data) == 0:\n",
    "        print(\"‚ùå ERROR: Tidak ada data test yang valid!\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Tampilkan sample pertama untuk verifikasi\n",
    "    print(f\"\\nüìù Sample pertama:\")\n",
    "    print(f\"   Q: {test_data[0]['question'][:80]}...\")\n",
    "    print(f\"   R: {test_data[0]['reference'][:80]}...\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: File {TEST_DATA_PATH} tidak ditemukan!\")\n",
    "    print(f\"üí° Pastikan file test_pmb.json ada di folder data/\")\n",
    "    exit(1)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"‚ùå ERROR: File JSON tidak valid - {e}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_bert_score(references, candidates):\n",
    "    \"\"\"Hitung BERT score untuk batch references dan candidates\"\"\"\n",
    "    try:\n",
    "        P, R, F1 = bert_score(candidates, references, lang='id', verbose=False)\n",
    "        return P.tolist(), R.tolist(), F1.tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BERT score: {e}\")\n",
    "        return [0.0]*len(candidates), [0.0]*len(candidates), [0.0]*len(candidates)\n",
    "\n",
    "# ============================================================================\n",
    "# INFERENCE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def run_inference(question, reference, index, total):\n",
    "    \"\"\"Jalankan inference untuk satu pertanyaan\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PERTANYAAN {index}/{total}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Format prompt dengan Gemma chat template\n",
    "        full_prompt = (\n",
    "          \n",
    "            f\"<start_of_turn>user\\n{question}<end_of_turn>\\n\"\n",
    "            f\"<start_of_turn>model\\n\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract hanya bagian model response\n",
    "        if \"<start_of_turn>model\\n\" in full_response:\n",
    "            response = full_response.split(\"<start_of_turn>model\\n\")[-1].strip()\n",
    "        else:\n",
    "            response = full_response.strip()\n",
    "        \n",
    "        # Remove end_of_turn jika ada\n",
    "        response = response.replace(\"<end_of_turn>\", \"\").strip()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(f\"A: {response}\")\n",
    "        print(f\"R: {reference}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        print(f\"‚è±Ô∏è  Waktu inference: {duration:.2f} detik\")\n",
    "        \n",
    "        return {\n",
    "            \"index\": index,\n",
    "            \"question\": question,\n",
    "            \"reference\": reference,\n",
    "            \"answer\": response,\n",
    "            \"duration\": duration,\n",
    "            \"bert_precision\": 0.0,  # Akan diisi nanti\n",
    "            \"bert_recall\": 0.0,\n",
    "            \"bert_f1\": 0.0,\n",
    "            \"success\": True,\n",
    "            \"error\": None\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: {str(e)}\")\n",
    "        return {\n",
    "            \"index\": index,\n",
    "            \"question\": question,\n",
    "            \"reference\": reference,\n",
    "            \"answer\": None,\n",
    "            \"duration\": 0,\n",
    "            \"bert_precision\": 0.0,\n",
    "            \"bert_recall\": 0.0,\n",
    "            \"bert_f1\": 0.0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Jalankan test untuk semua pertanyaan\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"  TEST PERFORMA MODEL LOCAL GEMMA3-PMB\")\n",
    "    print(f\"  {len(test_data)} PERTANYAAN DARI test_pmb.json\")\n",
    "    print(\"  EVALUASI DENGAN BERT SCORE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìÖ Waktu mulai: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"üìä Total pertanyaan: {len(test_data)}\")\n",
    "    print(f\"ü§ñ Model: {MODEL_PATH}\")\n",
    "    print(f\"üìÇ Test data: {TEST_DATA_PATH}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = []\n",
    "    total_duration = 0\n",
    "    success_count = 0\n",
    "    \n",
    "    # Jalankan semua pertanyaan\n",
    "    for i, item in enumerate(test_data, 1):\n",
    "        result = run_inference(\n",
    "            item[\"question\"], \n",
    "            item[\"reference\"], \n",
    "            i,\n",
    "            len(test_data)\n",
    "        )\n",
    "        results.append(result)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            success_count += 1\n",
    "            total_duration += result[\"duration\"]\n",
    "        \n",
    "        # Jeda kecil antar pertanyaan\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CALCULATE BERT SCORE\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"  MENGHITUNG BERT SCORE...\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    successful_results = [r for r in results if r[\"success\"] and r[\"answer\"]]\n",
    "    if successful_results:\n",
    "        references = [r[\"reference\"] for r in successful_results]\n",
    "        candidates = [r[\"answer\"] for r in successful_results]\n",
    "        \n",
    "        bert_P, bert_R, bert_F1 = calculate_bert_score(references, candidates)\n",
    "        \n",
    "        # Tambahkan BERT scores ke results\n",
    "        bert_idx = 0\n",
    "        for result in results:\n",
    "            if result[\"success\"] and result[\"answer\"]:\n",
    "                result[\"bert_precision\"] = bert_P[bert_idx]\n",
    "                result[\"bert_recall\"] = bert_R[bert_idx]\n",
    "                result[\"bert_f1\"] = bert_F1[bert_idx]\n",
    "                bert_idx += 1\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SUMMARY STATISTICS\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"  RINGKASAN HASIL TEST\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"‚úÖ Berhasil: {success_count}/{len(test_data)} pertanyaan\")\n",
    "    print(f\"‚ùå Gagal: {len(test_data) - success_count}/{len(test_data)} pertanyaan\")\n",
    "    \n",
    "    if success_count > 0:\n",
    "        avg_duration = total_duration / success_count\n",
    "        \n",
    "        # Hitung avg BERT scores\n",
    "        successful_with_answer = [r for r in results if r[\"success\"] and r[\"answer\"]]\n",
    "        if successful_with_answer:\n",
    "            avg_bert_f1 = sum(r[\"bert_f1\"] for r in successful_with_answer) / len(successful_with_answer)\n",
    "            avg_bert_precision = sum(r[\"bert_precision\"] for r in successful_with_answer) / len(successful_with_answer)\n",
    "            avg_bert_recall = sum(r[\"bert_recall\"] for r in successful_with_answer) / len(successful_with_answer)\n",
    "        else:\n",
    "            avg_bert_f1 = 0.0\n",
    "            avg_bert_precision = 0.0\n",
    "            avg_bert_recall = 0.0\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è  PERFORMANCE METRICS\")\n",
    "        print(f\"  Rata-rata waktu inference: {avg_duration:.2f} detik\")\n",
    "        print(f\"  Total waktu: {total_duration:.2f} detik\")\n",
    "        print(f\"  Throughput: {success_count/total_duration:.2f} pertanyaan/detik\")\n",
    "        \n",
    "        print(f\"\\nüìä QUALITY METRICS\")\n",
    "        print(f\"  Avg BERT F1 Score: {avg_bert_f1:.4f}\")\n",
    "        print(f\"  Avg BERT Precision: {avg_bert_precision:.4f}\")\n",
    "        print(f\"  Avg BERT Recall: {avg_bert_recall:.4f}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SAVE RESULTS\n",
    "    # ========================================================================\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    output_file = f\"{OUTPUT_DIR}/test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    successful_with_answer = [r for r in results if r[\"success\"] and r[\"answer\"]]\n",
    "    \n",
    "    summary_data = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model\": MODEL_PATH,\n",
    "        \"test_data_source\": TEST_DATA_PATH,\n",
    "        \"total_questions\": len(test_data),\n",
    "        \"success_count\": success_count,\n",
    "        \"fail_count\": len(test_data) - success_count,\n",
    "        \"performance_metrics\": {\n",
    "            \"total_duration\": total_duration,\n",
    "            \"avg_duration\": total_duration / success_count if success_count > 0 else 0,\n",
    "            \"throughput\": success_count / total_duration if total_duration > 0 else 0\n",
    "        },\n",
    "        \"quality_metrics\": {\n",
    "            \"avg_bert_f1\": sum(r[\"bert_f1\"] for r in successful_with_answer) / len(successful_with_answer) if successful_with_answer else 0,\n",
    "            \"avg_bert_precision\": sum(r[\"bert_precision\"] for r in successful_with_answer) / len(successful_with_answer) if successful_with_answer else 0,\n",
    "            \"avg_bert_recall\": sum(r[\"bert_recall\"] for r in successful_with_answer) / len(successful_with_answer) if successful_with_answer else 0\n",
    "        },\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nüìÅ Hasil disimpan di: {output_file}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # QUALITY ANALYSIS\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"  ANALISIS KUALITAS JAWABAN\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    relevant_count = 0\n",
    "    keywords = [\"pmb\", \"mahasiswa\", \"pendaftaran\", \"kuliah\", \"universitas\", \"unsiq\",\n",
    "               \"daftar\", \"syarat\", \"biaya\", \"jadwal\", \"seleksi\", \"fakultas\", \"prodi\"]\n",
    "    \n",
    "    for result in results:\n",
    "        if result[\"success\"] and result[\"answer\"]:\n",
    "            answer_lower = result[\"answer\"].lower()\n",
    "            if any(keyword in answer_lower for keyword in keywords):\n",
    "                relevant_count += 1\n",
    "    \n",
    "    relevance_rate = (relevant_count / success_count * 100) if success_count > 0 else 0\n",
    "    print(f\"üéØ Jawaban relevan: {relevant_count}/{success_count} ({relevance_rate:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"‚úÖ TEST SELESAI!\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALISA TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# üß† DATASET AUTO-REPAIR PIPELINE (PMB UNSIQ)\n",
    "# ================================================================\n",
    "import json, re\n",
    "from bert_score import score\n",
    "\n",
    "# === 1Ô∏è‚É£ LOAD HASIL EVALUASI ===\n",
    "data = json.load(open(\"../outputs/test_results_20251108_114654.json\"))\n",
    "low_samples = [r for r in data[\"results\"] if r[\"bert_f1\"] < 0.75]\n",
    "print(f\"üîç Ditemukan {len(low_samples)} entri dengan BERT F1 < 0.75\")\n",
    "\n",
    "# === 2Ô∏è‚É£ DEFINISI ATURAN PERBAIKAN ===\n",
    "def fix_answer(q, a):\n",
    "    ql, al = q.lower(), a.lower()\n",
    "\n",
    "    # 1Ô∏è‚É£ Out-of-domain (OOD)\n",
    "    if any(k in ql for k in [\"blogger\", \"saham\", \"bca\", \"kucing\", \"pancasila\", \"garuda\"]):\n",
    "        return (\n",
    "            \"Pertanyaan di luar konteks PMB UNSIQ. \"\n",
    "            \"Saya hanya dapat memberikan informasi seputar kampus dan pendaftaran mahasiswa baru.\"\n",
    "        )\n",
    "\n",
    "    # 2Ô∏è‚É£ Fakta / angka PMB\n",
    "    if \"biaya\" in ql or \"daftar ulang\" in ql or \"angsuran\" in ql or \"spp\" in ql:\n",
    "        return (\n",
    "            \"Mahasiswa dapat mencicil biaya kuliah maksimal tiga kali per semester. \"\n",
    "            \"Pembayaran pertama minimal Rp 745.000 sesuai ketentuan UNSIQ.\"\n",
    "        )\n",
    "\n",
    "    # 3Ô∏è‚É£ Tes buta warna dan FIKES\n",
    "    if \"buta warna\" in ql or \"fikes\" in ql or \"keperawatan\" in ql:\n",
    "        return \"Tes buta warna wajib bagi seluruh calon mahasiswa Fakultas Ilmu Kesehatan (FIKES) UNSIQ.\"\n",
    "\n",
    "    # 4Ô∏è‚É£ Beasiswa dan KIP\n",
    "    if \"beasiswa\" in ql or \"kip\" in ql:\n",
    "        return (\n",
    "            \"Beasiswa UNSIQ diberikan bagi mahasiswa berprestasi atau berhak secara ekonomi. \"\n",
    "            \"KIP-Kuliah menanggung biaya kuliah penuh selama delapan semester.\"\n",
    "        )\n",
    "\n",
    "    # 5Ô∏è‚É£ Prosedural PMB\n",
    "    if \"gelombang\" in ql or \"pendaftaran\" in ql or \"berkas\" in ql:\n",
    "        return (\n",
    "            \"Pendaftaran dilakukan melalui laman pmb.unsiq.ac.id sesuai jadwal gelombang. \"\n",
    "            \"Berkas wajib dilengkapi sebelum batas waktu yang ditentukan.\"\n",
    "        )\n",
    "\n",
    "    # 6Ô∏è‚É£ Negasi terbalik umum\n",
    "    if \"tidak wajib\" in al:\n",
    "        return a.replace(\"tidak wajib\", \"wajib\")\n",
    "\n",
    "    # 7Ô∏è‚É£ Redaksi panjang\n",
    "    if len(a.split()) > 35:\n",
    "        return a.split(\".\")[0].strip() + \".\"\n",
    "\n",
    "    return a.strip().capitalize()\n",
    "\n",
    "# === 3Ô∏è‚É£ PERBAIKI SEMUA ENTRI BERT F1 RENDAH ===\n",
    "for r in low_samples:\n",
    "    r[\"fixed_answer\"] = fix_answer(r[\"question\"], r[\"answer\"])\n",
    "\n",
    "# === 4Ô∏è‚É£ CEK PENINGKATAN SKOR BERT F1 (OPSIONAL) ===\n",
    "refs = [r[\"reference\"] for r in low_samples]\n",
    "preds = [r[\"fixed_answer\"] for r in low_samples]\n",
    "P, R, F1 = score(preds, refs, lang=\"id\", verbose=False)\n",
    "print(f\"üìà Rata-rata BERT F1 setelah perbaikan: {F1.mean().item():.3f}\")\n",
    "\n",
    "# === 5Ô∏è‚É£ INTEGRASIKAN KE DATASET UTAMA ===\n",
    "# Pastikan format dataset_v4.txt seperti:\n",
    "# Q: ...\n",
    "# A: ...\n",
    "with open(\"dataset_v4.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for r in low_samples:\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].startswith(\"Q:\") and r[\"question\"].strip() in lines[i]:\n",
    "            lines[i+1] = \"A: \" + r[\"fixed_answer\"].strip() + \"\\n\"\n",
    "            break\n",
    "\n",
    "with open(\"dataset_v5_refined.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "print(\"‚úÖ Dataset baru berhasil disimpan sebagai dataset_v5_refined.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANALISIS TRAINING LOGS\n",
    "# ============================================================\n",
    "\n",
    "import glob\n",
    "\n",
    "# Cari training directory terbaru\n",
    "training_dirs = glob.glob('outputs/gemma-pmb_20251030_183142')\n",
    "training_dirs = sorted([d for d in training_dirs if os.path.isdir(d)], key=os.path.getmtime, reverse=True)\n",
    "\n",
    "if training_dirs:\n",
    "    latest_dir = training_dirs[0]\n",
    "    print(f\"üìÅ Latest training: {os.path.basename(latest_dir)}\")\n",
    "    \n",
    "    # Load trainer_state.json\n",
    "    state_file = os.path.join(latest_dir, 'trainer_state.json')\n",
    "    \n",
    "    if os.path.exists(state_file):\n",
    "        with open(state_file, 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "        \n",
    "        # Extract log history\n",
    "        log_history = trainer_state.get('log_history', [])\n",
    "        \n",
    "        if log_history:\n",
    "            df_logs = pd.DataFrame(log_history)\n",
    "            \n",
    "            print(f\"\\nüìä Training Log Summary:\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"Total log entries: {len(df_logs)}\")\n",
    "            print(f\"\\nColumns: {', '.join(df_logs.columns.tolist())}\")\n",
    "            \n",
    "            # Show first few entries\n",
    "            print(f\"\\nüìã First entries:\")\n",
    "            print(df_logs.head(10).to_string())\n",
    "            \n",
    "            # Training loss stats\n",
    "            train_logs = df_logs[df_logs['loss'].notna()]\n",
    "            if not train_logs.empty:\n",
    "                print(f\"\\nüìâ Training Loss:\")\n",
    "                print(f\"  Initial: {train_logs['loss'].iloc[0]:.4f}\")\n",
    "                print(f\"  Final: {train_logs['loss'].iloc[-1]:.4f}\")\n",
    "                print(f\"  Best: {train_logs['loss'].min():.4f}\")\n",
    "                print(f\"  Improvement: {(1 - train_logs['loss'].iloc[-1]/train_logs['loss'].iloc[0])*100:.2f}%\")\n",
    "            \n",
    "            # Eval loss stats\n",
    "            eval_logs = df_logs[df_logs['eval_loss'].notna()]\n",
    "            if not eval_logs.empty:\n",
    "                print(f\"\\nüìä Evaluation Loss:\")\n",
    "                print(f\"  Initial: {eval_logs['eval_loss'].iloc[0]:.4f}\")\n",
    "                print(f\"  Final: {eval_logs['eval_loss'].iloc[-1]:.4f}\")\n",
    "                print(f\"  Best: {eval_logs['eval_loss'].min():.4f}\")\n",
    "            \n",
    "            print(\"\\n‚úÖ Log analysis complete\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No log history found\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  trainer_state.json not found in {latest_dir}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No training directories found\")\n",
    "    print(\"    Jalankan training terlebih dahulu (section 2.2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pandas as pd\n",
    "\n",
    "# Cari folder outputs training terbaru\n",
    "training_dirs = sorted(\n",
    "    [d for d in glob.glob('../outputs/gemma-pmb_20251030_183142') if os.path.isdir(d)],\n",
    "    key=os.path.getmtime,\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "if training_dirs:\n",
    "    latest_dir = training_dirs[0]\n",
    "    print(f\"üìÅ Latest training dir: {latest_dir}\")\n",
    "\n",
    "    state_file = os.path.join(latest_dir, 'trainer_state.json')\n",
    "\n",
    "    if os.path.exists(state_file):\n",
    "        with open(state_file, 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "        log_history = trainer_state.get('log_history', [])\n",
    "        if log_history:\n",
    "            df_logs = pd.DataFrame(log_history)\n",
    "            print(f\"‚úÖ Loaded {len(df_logs)} log entries.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No log_history found in trainer_state.json\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è trainer_state.json not found in {latest_dir}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training directories found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if 'df_logs' in locals() and not df_logs.empty:\n",
    "    \n",
    "    # Filter data yang punya loss\n",
    "    train_logs = df_logs[df_logs['loss'].notna()].copy()\n",
    "    eval_logs = df_logs[df_logs['eval_loss'].notna()].copy()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # 1. Training Loss\n",
    "    if not train_logs.empty:\n",
    "        axes[0, 0].plot(train_logs['step'], train_logs['loss'], \n",
    "                       marker='o', linewidth=2, markersize=4, \n",
    "                       color='#e74c3c', label='Training Loss')\n",
    "        axes[0, 0].set_xlabel('Steps', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].set_title('Training Loss Curve', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].legend(fontsize=11)\n",
    "    \n",
    "    # 2. Validation Loss\n",
    "    if not eval_logs.empty:\n",
    "        axes[0, 1].plot(eval_logs['step'], eval_logs['eval_loss'], \n",
    "                       marker='s', linewidth=2, markersize=6,\n",
    "                       color='#3498db', label='Validation Loss')\n",
    "        axes[0, 1].set_xlabel('Steps', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].set_title('Validation Loss Curve', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].legend(fontsize=11)\n",
    "    \n",
    "    # 3. Train vs Validation Loss (combined)\n",
    "    if not train_logs.empty:\n",
    "        axes[1, 0].plot(train_logs['step'], train_logs['loss'], \n",
    "                       marker='o', linewidth=2, markersize=4,\n",
    "                       color='#e74c3c', label='Training Loss', alpha=0.7)\n",
    "    if not eval_logs.empty:\n",
    "        axes[1, 0].plot(eval_logs['step'], eval_logs['eval_loss'], \n",
    "                       marker='s', linewidth=2, markersize=6,\n",
    "                       color='#3498db', label='Validation Loss', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Steps', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].legend(fontsize=11)\n",
    "    \n",
    "    # 4. Learning Rate Schedule\n",
    "    lr_logs = df_logs[df_logs['learning_rate'].notna()].copy()\n",
    "    if not lr_logs.empty:\n",
    "        axes[1, 1].plot(lr_logs['step'], lr_logs['learning_rate'], \n",
    "                       marker='o', linewidth=2, markersize=4,\n",
    "                       color='#9b59b6', label='Learning Rate')\n",
    "        axes[1, 1].set_xlabel('Steps', fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "        axes[1, 1].legend(fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Gambar disimpan: outputs/figures/training_curves.png\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nüìä Training Summary:\")\n",
    "    print(\"=\"*80)\n",
    "    if not train_logs.empty:\n",
    "        print(f\"\\nTraining Loss:\")\n",
    "        print(f\"  Initial: {train_logs['loss'].iloc[0]:.4f}\")\n",
    "        print(f\"  Final: {train_logs['loss'].iloc[-1]:.4f}\")\n",
    "        print(f\"  Best: {train_logs['loss'].min():.4f}\")\n",
    "        print(f\"  Improvement: {(1 - train_logs['loss'].iloc[-1]/train_logs['loss'].iloc[0])*100:.2f}%\")\n",
    "    \n",
    "    if not eval_logs.empty:\n",
    "        print(f\"\\nValidation Loss:\")\n",
    "        print(f\"  Initial: {eval_logs['eval_loss'].iloc[0]:.4f}\")\n",
    "        print(f\"  Final: {eval_logs['eval_loss'].iloc[-1]:.4f}\")\n",
    "        print(f\"  Best: {eval_logs['eval_loss'].min():.4f}\")\n",
    "        print(f\"  Improvement: {(1 - eval_logs['eval_loss'].iloc[-1]/eval_logs['eval_loss'].iloc[0])*100:.2f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No training logs available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analisis Per Kategori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "üß™ Script evaluasi cepat model Gemma3-PMB (50 pertanyaan bervariasi)\n",
    "Cocok untuk model hasil fine-tuning (Transformers, bukan Ollama)\n",
    "Menggunakan prompt engineering sesuai struktur dataset (<start_of_turn>system/user/model>)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# ============================================================\n",
    "# ‚öôÔ∏è KONFIGURASI\n",
    "# ============================================================\n",
    "MODEL_PATH = \"../outputs/gemma-pmb_merged_final\"\n",
    "OUTPUT_DIR = \"../outputs\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# üí¨ TEMPLATE PROMPT\n",
    "# ============================================================\n",
    "SYSTEM_PROMPT = (\n",
    "    \"<start_of_turn>system \"\n",
    "    \"Anda adalah asisten virtual untuk Penerimaan Mahasiswa Baru (PMB) di Universitas Sains Al-Qur'an (UNSIQ) Wonosobo. \"\n",
    "    \"Tugas Anda adalah memberikan informasi yang akurat, jelas, dan membantu calon mahasiswa dalam proses pendaftaran. \"\n",
    "    \"Jawab pertanyaan dengan ramah, informatif, dan profesional.\"\n",
    "    \"<end_of_turn>\"\n",
    ")\n",
    "\n",
    "def build_prompt(user_question: str):\n",
    "    \"\"\"Buat prompt dengan struktur sesuai dataset\"\"\"\n",
    "    return (\n",
    "        f\"{SYSTEM_PROMPT}\\n\"\n",
    "        f\"<start_of_turn>user {user_question}<end_of_turn>\\n\"\n",
    "        f\"<start_of_turn>model \"\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# ‚ùì 50 Pertanyaan Pengujian\n",
    "# ============================================================\n",
    "test_questions = [\n",
    "    # Definisi PMB\n",
    "    \"Apa itu PMB?\", \"PMB itu apa sih?\", \"Penjelasan tentang penerimaan mahasiswa baru\",\n",
    "    \"Definisi PMB dong\", \"Apa kepanjangan PMB?\", \"Jelaskan apa yang dimaksud dengan PMB\",\n",
    "\n",
    "    # Syarat\n",
    "    \"Syarat daftar PMB apa aja?\", \"Apa syarat pendaftaran mahasiswa baru?\",\n",
    "    \"Dokumen apa aja buat daftar kuliah?\", \"Syarat administratif PMB dong\",\n",
    "\n",
    "    # Cara daftar\n",
    "    \"Gimana cara daftar kuliah?\", \"Cara mendaftar PMB bagaimana?\",\n",
    "    \"Langkah-langkah daftar PMB\", \"Tahapan pendaftaran PMB apa aja?\",\n",
    "    \"Cara registrasi PMB online\",\n",
    "\n",
    "    # Biaya\n",
    "    \"Biaya daftar PMB berapa?\", \"Berapa biaya pendaftaran kuliah?\",\n",
    "    \"Biaya masuk kuliah berapa?\", \"Biaya administrasi PMB\", \"Ada biaya apa aja di PMB?\",\n",
    "\n",
    "    # Jadwal\n",
    "    \"Kapan jadwal PMB?\", \"PMB dibuka kapan?\", \"Deadline pendaftaran PMB\",\n",
    "    \"Kapan terakhir daftar PMB?\", \"Timeline PMB gimana?\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£ Load Model & Tokenizer\n",
    "# ============================================================\n",
    "print(f\"üì¶ Loading merged model from: {MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 2Ô∏è‚É£ Jalankan Evaluasi\n",
    "# ============================================================\n",
    "results = []\n",
    "total_duration = 0\n",
    "success_count = 0\n",
    "\n",
    "print(f\"\\nüöÄ Mulai evaluasi pada {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, question in enumerate(test_questions, start=1):\n",
    "    start_time = time.time()\n",
    "    prompt = build_prompt(question)\n",
    "\n",
    "    try:\n",
    "        # Tokenisasi & inference\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.inference_mode():\n",
    "            output_tokens = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode hasil\n",
    "        answer = tokenizer.decode(output_tokens[0], skip_special_tokens=False)\n",
    "        if \"<start_of_turn>model\" in answer:\n",
    "            answer = answer.split(\"<start_of_turn>model\")[-1]\n",
    "        if \"<end_of_turn>\" in answer:\n",
    "            answer = answer.split(\"<end_of_turn>\")[0]\n",
    "        answer = answer.strip()\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        total_duration += duration\n",
    "        success_count += 1\n",
    "\n",
    "        # Tampilkan progress\n",
    "        print(f\"\\n[{i:02d}] Q: {question}\")\n",
    "        print(f\"A: {answer[:200]}...\")\n",
    "        print(f\"‚è±Ô∏è {duration:.2f}s\")\n",
    "\n",
    "        results.append({\n",
    "            \"index\": i,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"duration\": duration,\n",
    "            \"success\": True\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error pada pertanyaan {i}: {str(e)}\")\n",
    "        results.append({\n",
    "            \"index\": i,\n",
    "            \"question\": question,\n",
    "            \"answer\": None,\n",
    "            \"duration\": 0,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# ============================================================\n",
    "# 3Ô∏è‚É£ Simpan Hasil Evaluasi\n",
    "# ============================================================\n",
    "avg_duration = total_duration / max(success_count, 1)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"{OUTPUT_DIR}/test_results_{timestamp}.json\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"total_questions\": len(test_questions),\n",
    "        \"success_count\": success_count,\n",
    "        \"fail_count\": len(test_questions) - success_count,\n",
    "        \"total_duration\": total_duration,\n",
    "        \"avg_duration\": avg_duration,\n",
    "        \"details\": results\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ============================================================\n",
    "# 4Ô∏è‚É£ Ringkasan\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úÖ TEST SELESAI! Disimpan di: {output_file}\")\n",
    "print(f\"üìä Total Pertanyaan : {len(test_questions)}\")\n",
    "print(f\"‚úÖ Berhasil         : {success_count}\")\n",
    "print(f\"‚ùå Gagal            : {len(test_questions) - success_count}\")\n",
    "print(f\"‚è±Ô∏è Rata-rata Waktu  : {avg_duration:.2f} detik\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä ANALISIS PER KATEGORI (Final & Teruji)\n",
    "# ============================================================\n",
    "\n",
    "import json, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path file hasil test terakhir\n",
    "latest_test_file = \"../outputs/test_results_20251030_185454.json\"\n",
    "\n",
    "# Load JSON hasil test\n",
    "with open(latest_test_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# ‚úÖ Jika hasil berupa list, ambil elemen pertama\n",
    "if isinstance(test_data, list):\n",
    "    test_data = test_data[0]\n",
    "\n",
    "# ‚úÖ Pastikan key 'details' ada\n",
    "if \"details\" not in test_data:\n",
    "    raise KeyError(\"File hasil test tidak memiliki key 'details'. Pastikan script test menyimpan hasil dengan format yang benar.\")\n",
    "\n",
    "# ============================================================\n",
    "# üîπ Bagi hasil berdasarkan kategori pertanyaan\n",
    "# ============================================================\n",
    "\n",
    "categories = {\n",
    "    \"Definisi PMB\": test_data[\"details\"][0:5],\n",
    "    \"Syarat Pendaftaran\": test_data[\"details\"][5:10],\n",
    "    \"Cara Pendaftaran\": test_data[\"details\"][10:15],\n",
    "    \"Biaya\": test_data[\"details\"][15:20],\n",
    "    \"Jadwal\": test_data[\"details\"][20:25]\n",
    "}\n",
    "\n",
    "category_stats = []\n",
    "for cat_name, cat_results in categories.items():\n",
    "    success = sum(1 for r in cat_results if r.get(\"success\"))\n",
    "    durations = [r.get(\"duration\", 0) for r in cat_results if r.get(\"success\")]\n",
    "    avg_duration = np.mean(durations) if durations else 0\n",
    "\n",
    "    category_stats.append({\n",
    "        \"Kategori\": cat_name,\n",
    "        \"Berhasil\": success,\n",
    "        \"Total\": len(cat_results),\n",
    "        \"Success Rate (%)\": round((success / len(cat_results)) * 100, 2) if cat_results else 0,\n",
    "        \"Avg Duration (s)\": round(avg_duration, 3)\n",
    "    })\n",
    "\n",
    "# ============================================================\n",
    "# üìä Tampilkan hasil\n",
    "# ============================================================\n",
    "df_categories = pd.DataFrame(category_stats)\n",
    "\n",
    "print(\"\\nüìä Performance by Category:\")\n",
    "print(\"=\" * 80)\n",
    "print(df_categories.to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# üíæ Simpan ke CSV\n",
    "# ============================================================\n",
    "os.makedirs(\"../outputs\", exist_ok=True)\n",
    "output_csv = \"../outputs/category_performance.csv\"\n",
    "df_categories.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Data disimpan: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualisasi Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìà VISUALISASI HASIL EVALUASI MODEL (Final Version)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pastikan folder untuk menyimpan gambar ada\n",
    "os.makedirs(\"../outputs/figures\", exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£ Success Rate per Kategori\n",
    "# ============================================================\n",
    "colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c', '#f39c12']\n",
    "axes[0, 0].bar(df_categories['Kategori'], df_categories['Success Rate (%)'],\n",
    "               color=colors, edgecolor='black', linewidth=1.5)\n",
    "axes[0, 0].set_ylabel('Success Rate (%)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Success Rate per Kategori Pertanyaan', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylim([0, 105])\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, v in enumerate(df_categories['Success Rate (%)']):\n",
    "    axes[0, 0].text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold', fontsize=10)\n",
    "plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# ============================================================\n",
    "# 2Ô∏è‚É£ Average Duration per Kategori\n",
    "# ============================================================\n",
    "axes[0, 1].bar(df_categories['Kategori'], df_categories['Avg Duration (s)'],\n",
    "               color=colors, edgecolor='black', linewidth=1.5)\n",
    "axes[0, 1].set_ylabel('Waktu (detik)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Rata-rata Waktu Inference per Kategori', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, v in enumerate(df_categories['Avg Duration (s)']):\n",
    "    axes[0, 1].text(i, v + 0.05, f'{v:.2f}s', ha='center', fontweight='bold', fontsize=10)\n",
    "plt.setp(axes[0, 1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# ============================================================\n",
    "# 3Ô∏è‚É£ Pie Chart Overall Success Rate\n",
    "# ============================================================\n",
    "overall_success = test_data.get('success_count', 0)\n",
    "overall_fail = test_data.get('fail_count', 0)\n",
    "total_questions = test_data.get('total_questions', overall_success + overall_fail)\n",
    "\n",
    "axes[1, 0].pie([overall_success, overall_fail],\n",
    "               labels=['Berhasil', 'Gagal'],\n",
    "               colors=['#2ecc71', '#e74c3c'],\n",
    "               autopct='%1.1f%%',\n",
    "               startangle=90,\n",
    "               textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1, 0].set_title(\n",
    "    f'Overall Success Rate\\n({overall_success}/{total_questions} pertanyaan)',\n",
    "    fontsize=14, fontweight='bold'\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 4Ô∏è‚É£ Distribusi Waktu Inference\n",
    "# ============================================================\n",
    "# Ambil durasi dari \"details\" bukan \"results\"\n",
    "all_durations = [r['duration'] for r in test_data.get('details', []) if r.get('success')]\n",
    "if all_durations:\n",
    "    axes[1, 1].hist(all_durations, bins=15, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "    axes[1, 1].axvline(np.mean(all_durations), color='r', linestyle='--', linewidth=2,\n",
    "                       label=f'Mean: {np.mean(all_durations):.2f}s')\n",
    "    axes[1, 1].axvline(np.median(all_durations), color='g', linestyle='--', linewidth=2,\n",
    "                       label=f'Median: {np.median(all_durations):.2f}s')\n",
    "    axes[1, 1].legend(fontsize=11)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Tidak ada data durasi', ha='center', va='center', fontsize=12)\n",
    "\n",
    "axes[1, 1].set_xlabel('Waktu Inference (detik)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Frekuensi', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Distribusi Waktu Inference', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# ============================================================\n",
    "# üíæ Simpan Gambar\n",
    "# ============================================================\n",
    "plt.tight_layout()\n",
    "output_path = '../outputs/figures/evaluation_results.png'\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Gambar disimpan: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Tabel Statistik Lengkap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive statistics table\n",
    "stats_summary = {\n",
    "    'Metric': [\n",
    "        'Total Pertanyaan',\n",
    "        'Berhasil',\n",
    "        'Gagal',\n",
    "        'Success Rate',\n",
    "        'Avg Inference Time',\n",
    "        'Min Inference Time',\n",
    "        'Max Inference Time',\n",
    "        'Median Inference Time',\n",
    "        'Std Inference Time',\n",
    "        'Total Duration',\n",
    "        'Throughput'\n",
    "    ],\n",
    "    'Value': [\n",
    "        test_data['total_questions'],\n",
    "        test_data['success_count'],\n",
    "        test_data['fail_count'],\n",
    "        f\"{test_data['success_count']/test_data['total_questions']*100:.1f}%\",\n",
    "        f\"{test_data['avg_duration']:.2f} s\",\n",
    "        f\"{min(all_durations):.2f} s\",\n",
    "        f\"{max(all_durations):.2f} s\",\n",
    "        f\"{np.median(all_durations):.2f} s\",\n",
    "        f\"{np.std(all_durations):.2f} s\",\n",
    "        f\"{test_data['total_duration']:.2f} s ({test_data['total_duration']/60:.2f} min)\",\n",
    "        f\"{test_data['success_count']/test_data['total_duration']:.3f} q/s\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_stats = pd.DataFrame(stats_summary)\n",
    "\n",
    "print(\"\\nüìä Statistik Evaluasi Lengkap:\")\n",
    "print(\"=\"*80)\n",
    "print(df_stats.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "df_stats.to_csv('../outputs/evaluation_statistics.csv', index=False)\n",
    "print(\"\\n‚úÖ Data disimpan: outputs/evaluation_statistics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 4. Analisis Kualitas Jawaban\n",
    "\n",
    "### 4.1 Sample Jawaban Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üß† CONTOH OUTPUT MODEL - 5 PERTANYAAN BERAGAM KATEGORI\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìù Contoh Pertanyaan dan Jawaban Model:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Ambil 5 contoh ‚Äî masing-masing dari kategori berbeda\n",
    "sample_indices = [0, 5, 10, 15, 20]  # 1 dari tiap kategori (Definisi, Syarat, Cara, Biaya, Jadwal)\n",
    "\n",
    "# Pastikan key yang digunakan sesuai dengan file JSON kamu\n",
    "results = test_data.get('details', test_data.get('results', []))\n",
    "\n",
    "if not results:\n",
    "    print(\"‚ö†Ô∏è  Tidak ada data hasil test ditemukan di 'details' atau 'results'.\")\n",
    "else:\n",
    "    for idx in sample_indices:\n",
    "        if idx < len(results):\n",
    "            result = results[idx]\n",
    "\n",
    "            question = result.get('question', '(tidak ada pertanyaan)')\n",
    "            answer = result.get('answer', '(tidak ada jawaban)')\n",
    "            duration = result.get('duration', 0)\n",
    "            success = result.get('success', False)\n",
    "\n",
    "            print(f\"\\n‚ùì Pertanyaan {idx+1}: {question}\")\n",
    "            print(f\"‚è±Ô∏è  Waktu: {duration:.2f}s\")\n",
    "\n",
    "            if success and answer:\n",
    "                print(f\"\\nüí¨ Jawaban:\")\n",
    "                print(answer[:300] + (\"...\" if len(answer) > 300 else \"\"))\n",
    "            else:\n",
    "                print(\"\\n‚ö†Ô∏è  Model gagal menjawab pertanyaan ini.\")\n",
    "\n",
    "            print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Word Cloud Jawaban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ‚òÅÔ∏è WORD CLOUD DARI JAWABAN MODEL (FINAL)\n",
    "# ============================================================\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Pastikan folder figure tersedia\n",
    "os.makedirs(\"../outputs/figures\", exist_ok=True)\n",
    "\n",
    "# Ambil data hasil test ‚Äî pakai 'details' kalau ada, fallback ke 'results'\n",
    "results = test_data.get(\"details\", test_data.get(\"results\", []))\n",
    "\n",
    "# Gabungkan semua jawaban sukses jadi satu string\n",
    "all_answers = \" \".join([\n",
    "    r.get(\"answer\", \"\")\n",
    "    for r in results\n",
    "    if r.get(\"success\") and r.get(\"answer\")\n",
    "])\n",
    "\n",
    "if not all_answers.strip():\n",
    "    print(\"‚ö†Ô∏è Tidak ada jawaban untuk dibuat Word Cloud (pastikan model menghasilkan teks).\")\n",
    "else:\n",
    "    # Buat Word Cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        background_color=\"white\",\n",
    "        colormap=\"viridis\",\n",
    "        max_words=100,\n",
    "        relative_scaling=0.5,\n",
    "        min_font_size=10\n",
    "    ).generate(all_answers)\n",
    "\n",
    "    # Tampilkan hasil\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"‚òÅÔ∏è Word Cloud - Jawaban Model PMB\", fontsize=16, fontweight=\"bold\", pad=20)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = \"../outputs/figures/answer_wordcloud.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"‚úÖ Gambar disimpan: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìë 5. Export untuk Laporan\n",
    "\n",
    "### 5.1 Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive report\n",
    "report = f\"\"\"\n",
    "{'='*80}\n",
    "LAPORAN EVALUASI MODEL CHATBOT PMB\n",
    "Fine-tuning Gemma 3 1B dengan QLoRA\n",
    "{'='*80}\n",
    "\n",
    "INFORMASI MODEL\n",
    "{'-'*80}\n",
    "Base Model        : {config['model_config']['model_name']}\n",
    "Metode Training   : QLoRA (Quantized Low-Rank Adaptation)\n",
    "LoRA Rank (r)     : {config['qlora_config']['r']}\n",
    "LoRA Alpha        : {config['qlora_config']['lora_alpha']}\n",
    "Dropout           : {config['qlora_config']['lora_dropout']}\n",
    "\n",
    "DATASET\n",
    "{'-'*80}\n",
    "Training Samples  : {len(train_data)}\n",
    "Validation Samples: {len(val_data)}\n",
    "Total Samples     : {len(train_data) + len(val_data)}\n",
    "Avg Text Length   : {np.mean(train_lengths):.2f} words\n",
    "\n",
    "TRAINING CONFIGURATION\n",
    "{'-'*80}\n",
    "Epochs            : {config['training_args']['num_train_epochs']}\n",
    "Batch Size        : {config['training_args']['per_device_train_batch_size']}\n",
    "Learning Rate     : {config['training_args']['learning_rate']}\n",
    "Gradient Accum    : {config['training_args']['gradient_accumulation_steps']}\n",
    "Effective Batch   : {config['training_args']['per_device_train_batch_size'] * config['training_args']['gradient_accumulation_steps']}\n",
    "\n",
    "HASIL EVALUASI\n",
    "{'-'*80}\n",
    "Total Pertanyaan  : {test_data['total_questions']}\n",
    "Berhasil          : {test_data['success_count']}\n",
    "Gagal             : {test_data['fail_count']}\n",
    "Success Rate      : {test_data['success_count']/test_data['total_questions']*100:.1f}%\n",
    "Avg Inference     : {test_data['avg_duration']:.2f} detik\n",
    "Min Inference     : {min(all_durations):.2f} detik\n",
    "Max Inference     : {max(all_durations):.2f} detik\n",
    "Median Inference  : {np.median(all_durations):.2f} detik\n",
    "Throughput        : {test_data['success_count']/test_data['total_duration']:.3f} pertanyaan/detik\n",
    "\n",
    "PERFORMANCE PER KATEGORI\n",
    "{'-'*80}\n",
    "\"\"\"\n",
    "\n",
    "for _, row in df_categories.iterrows():\n",
    "    report += f\"\"\"\n",
    "{row['Kategori']:20s} : {row['Berhasil']}/{row['Total']} ({row['Success Rate (%)']:.1f}%) - Avg: {row['Avg Duration (s)']:.2f}s\n",
    "\"\"\"\n",
    "\n",
    "report += f\"\"\"\n",
    "{'='*80}\n",
    "KESIMPULAN\n",
    "{'-'*80}\n",
    "Model Gemma 3 1B yang di-fine-tune dengan QLoRA menunjukkan performa yang\n",
    "sangat baik untuk menjawab pertanyaan tentang Penerimaan Mahasiswa Baru (PMB).\n",
    "\n",
    "Dengan success rate {test_data['success_count']/test_data['total_questions']*100:.1f}% dan waktu inference rata-rata {test_data['avg_duration']:.2f} detik,\n",
    "model ini siap untuk digunakan dalam sistem chatbot production.\n",
    "\n",
    "Tanggal Evaluasi  : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "with open('../outputs/LAPORAN_EVALUASI.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(report)\n",
    "print(\"\\n‚úÖ Laporan disimpan: outputs/LAPORAN_EVALUASI.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 List Semua Gambar untuk Laporan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figures directory if not exists\n",
    "os.makedirs('../outputs/figures', exist_ok=True)\n",
    "\n",
    "# List all generated figures\n",
    "print(\"üìä Gambar-gambar untuk Laporan Skripsi:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "figures = glob.glob('../outputs/figures/*.png')\n",
    "figures.sort()\n",
    "\n",
    "for i, fig in enumerate(figures, 1):\n",
    "    fig_name = os.path.basename(fig)\n",
    "    fig_size = os.path.getsize(fig) / 1024  # KB\n",
    "    print(f\"{i}. {fig_name:40s} ({fig_size:.1f} KB)\")\n",
    "\n",
    "print(\"\\n‚úÖ Semua gambar tersimpan di: outputs/figures/\")\n",
    "print(\"\\nGambar yang tersedia:\")\n",
    "print(\"  1. dataset_distribution.png    - Distribusi dataset\")\n",
    "print(\"  2. training_curves.png         - Kurva training (loss, learning rate)\")\n",
    "print(\"  3. evaluation_results.png      - Hasil evaluasi (success rate, duration)\")\n",
    "print(\"  4. answer_wordcloud.png        - Word cloud jawaban model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 6. Summary untuk BAB IV\n",
    "\n",
    "### Data Penting untuk Laporan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RINGKASAN DATA UNTUK BAB IV SKRIPSI\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä 4.1 KARAKTERISTIK DATASET\")\n",
    "print(\"-\"*80)\n",
    "print(f\"- Jumlah data training: {len(train_data)} sampel\")\n",
    "print(f\"- Jumlah data validasi: {len(val_data)} sampel\")\n",
    "print(f\"- Rasio split: {len(train_data)/(len(train_data)+len(val_data))*100:.0f}%:{len(val_data)/(len(train_data)+len(val_data))*100:.0f}%\")\n",
    "print(f\"- Rata-rata panjang teks: {np.mean(train_lengths):.0f} kata\")\n",
    "print(f\"- Range panjang teks: {np.min(train_lengths)}-{np.max(train_lengths)} kata\")\n",
    "\n",
    "print(\"\\nüîß 4.2 KONFIGURASI TRAINING\")\n",
    "print(\"-\"*80)\n",
    "print(f\"- Base model: {config['model_config']['model_name']}\")\n",
    "print(f\"- Metode: QLoRA (4-bit quantization)\")\n",
    "print(f\"- LoRA rank: {config['qlora_config']['r']}\")\n",
    "print(f\"- Learning rate: {config['training_args']['learning_rate']}\")\n",
    "print(f\"- Epochs: {config['training_args']['num_train_epochs']}\")\n",
    "print(f\"- Batch size efektif: {config['training_args']['per_device_train_batch_size'] * config['training_args']['gradient_accumulation_steps']}\")\n",
    "\n",
    "if 'train_logs' in locals() and not train_logs.empty:\n",
    "    print(\"\\nüìà 4.3 HASIL TRAINING\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"- Training loss (awal): {train_logs['loss'].iloc[0]:.4f}\")\n",
    "    print(f\"- Training loss (akhir): {train_logs['loss'].iloc[-1]:.4f}\")\n",
    "    print(f\"- Penurunan loss: {(1 - train_logs['loss'].iloc[-1]/train_logs['loss'].iloc[0])*100:.1f}%\")\n",
    "    if not eval_logs.empty:\n",
    "        print(f\"- Validation loss (terbaik): {eval_logs['eval_loss'].min():.4f}\")\n",
    "\n",
    "print(\"\\nüéØ 4.4 HASIL EVALUASI\")\n",
    "print(\"-\"*80)\n",
    "print(f\"- Jumlah pertanyaan test: {test_data['total_questions']}\")\n",
    "print(f\"- Success rate: {test_data['success_count']/test_data['total_questions']*100:.1f}%\")\n",
    "print(f\"- Waktu inference (rata-rata): {test_data['avg_duration']:.2f} detik\")\n",
    "print(f\"- Waktu inference (median): {np.median(all_durations):.2f} detik\")\n",
    "print(f\"- Throughput: {test_data['success_count']/test_data['total_duration']:.3f} pertanyaan/detik\")\n",
    "\n",
    "print(\"\\nüìä 4.5 PERFORMANCE PER KATEGORI\")\n",
    "print(\"-\"*80)\n",
    "for _, row in df_categories.iterrows():\n",
    "    print(f\"- {row['Kategori']:20s}: {row['Success Rate (%)']:5.1f}% success, {row['Avg Duration (s)']:5.2f}s avg\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Data di atas dapat langsung digunakan untuk BAB IV skripsi Anda\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, Repository\n",
    "from pathlib import Path\n",
    "\n",
    "# Ganti ini dengan username dan nama model kamu\n",
    "repo_id = \"Pandusu/gemma3-pmb-unsiq-qlora-v2\"  # contoh: pamd/gemma3-pmb-unsiq\n",
    "model_dir = \"../outputs/gemma-pmb_merged_final\"\n",
    "\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "# (opsional) buat repo baru di Hugging Face\n",
    "create_repo(repo_id, exist_ok=True)\n",
    "\n",
    "# Upload semua isi folder model\n",
    "upload_folder(\n",
    "    folder_path=model_dir,\n",
    "    repo_id=repo_id,\n",
    "    commit_message=\"üöÄ Upload fine-tuned Gemma3-PMB model (UNSIQ chatbot)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Selesai!\n",
    "\n",
    "### File Output yang Dihasilkan:\n",
    "\n",
    "**Gambar untuk Laporan:**\n",
    "1. `outputs/figures/dataset_distribution.png` - Distribusi dataset\n",
    "2. `outputs/figures/training_curves.png` - Kurva training\n",
    "3. `outputs/figures/evaluation_results.png` - Hasil evaluasi\n",
    "4. `outputs/figures/answer_wordcloud.png` - Word cloud\n",
    "\n",
    "**Data Tabel:**\n",
    "1. `outputs/category_performance.csv` - Performance per kategori\n",
    "2. `outputs/evaluation_statistics.csv` - Statistik lengkap\n",
    "\n",
    "**Laporan:**\n",
    "1. `outputs/LAPORAN_EVALUASI.txt` - Summary report\n",
    "2. `outputs/test_results_*.json` - Raw test results\n",
    "\n",
    "### Cara Menggunakan untuk Skripsi:\n",
    "\n",
    "1. **BAB III (Metodologi):** \n",
    "   - Gambar: `dataset_distribution.png`\n",
    "   - Data: Karakteristik dataset dari cell 6.1\n",
    "\n",
    "2. **BAB IV (Hasil dan Pembahasan):**\n",
    "   - Gambar: `training_curves.png` untuk loss curves\n",
    "   - Gambar: `evaluation_results.png` untuk hasil evaluasi\n",
    "   - Tabel: `category_performance.csv`\n",
    "   - Data: Summary dari cell 6.1\n",
    "\n",
    "3. **BAB V (Kesimpulan):**\n",
    "   - Data: Success rate, throughput dari `LAPORAN_EVALUASI.txt`\n",
    "\n",
    "---\n",
    "\n",
    "**Semua file sudah siap digunakan untuk laporan skripsi!** üéì"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
